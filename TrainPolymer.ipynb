{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae7d7630-3f66-4736-bcbe-45cce9ef41e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/code/polymer/tfrecords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e84c21-4993-41ea-a134-407ebdd56f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import jax\n",
    "import optax\n",
    "import hashlib\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from flax import struct\n",
    "from flax.training import train_state\n",
    "from tqdm.notebook import tqdm  # Better for Jupyter\n",
    "import time\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "\n",
    "from typing import NamedTuple, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "487bf499-d142-4f61-a4f3-35b07efc4c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstructions(state, cached_images, cached_waves, key, num_samples=6):\n",
    "\n",
    "    # Select random samples from cached batch\n",
    "    key, subkey = random.split(key)\n",
    "    batch_size = cached_images.shape[0]\n",
    "    num_samples = min(num_samples, batch_size)\n",
    "    indices = random.choice(subkey, batch_size, (num_samples,), replace=False)\n",
    "    original_images = cached_images[indices]\n",
    "    \n",
    "    # Get reconstructions\n",
    "    key, subkey = random.split(key)\n",
    "    reconstructed_images, _, _, _ = reconstruct_batch(state, cached_waves[indices], subkey)\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    original_np = np.array(original_images)\n",
    "    reconstructed_np = np.array(reconstructed_images)\n",
    "\n",
    "    fig, axs = plt.subplots(num_samples, 2)\n",
    "    for k in range(num_samples):\n",
    "        axs[k, 0].imshow(original_np[k, ...])\n",
    "        axs[k, 1].imshow(reconstructed_np[k, ...])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4060d3f9-e806-4ba0-8270-49c9edf6dee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Checking GPU availability...\n",
      "JAX devices: [CudaDevice(id=0)]\n",
      "JAX default backend: gpu\n",
      "✅ Found 1 GPU(s): [CudaDevice(id=0)]\n",
      "🎯 Setting JAX to use GPU...\n",
      "Initializing VAE model...\n",
      "Model initialized!\n",
      "📋 Training Configuration:\n",
      "   Learning Rate: 0.001\n",
      "   Epochs: 20\n",
      "   Batch Size: 32\n",
      "\n",
      "📂 Setting up dataloader...\n",
      "Found 1 TFRecord files\n",
      "Will generate 16 crops of 256x256 per image\n",
      "Dataloader ready!\n",
      "\n",
      "Starting training loop...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb57b2ed5bb84bd585dd9b700a97f8f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adaptive Training Progress:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 138\u001b[39m\n\u001b[32m    136\u001b[39m permuted_waves = jax.lax.stop_gradient(permuted_waves)\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# Train step\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m state, total_loss = \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpermuted_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpermuted_waves\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey2\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m batch_time = time.time() - batch_start\n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# print('LOSSES:', total_loss)\u001b[39;00m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py:270\u001b[39m, in \u001b[36m_cpp_pjit.<locals>.cache_miss\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.no_tracing.value:\n\u001b[32m    266\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mre-tracing function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjit_info.fun_sourceinfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    267\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m`jit`, but \u001b[39m\u001b[33m'\u001b[39m\u001b[33mno_tracing\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is set\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    269\u001b[39m (outs, out_flat, out_tree, args_flat, jaxpr,\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m  executable, pgle_profiler, const_args) = \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m     \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m maybe_fastpath_data = _get_fastpath_data(\n\u001b[32m    274\u001b[39m     executable, out_tree, args_flat, out_flat, jaxpr.effects, jaxpr.consts,\n\u001b[32m    275\u001b[39m     jit_info.abstracted_axes, pgle_profiler,\n\u001b[32m    276\u001b[39m     const_args)\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outs, maybe_fastpath_data, _need_to_rebuild_with_fdo(pgle_profiler)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py:149\u001b[39m, in \u001b[36m_python_pjit_helper\u001b[39m\u001b[34m(fun, jit_info, *args, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m   args_flat = \u001b[38;5;28mmap\u001b[39m(core.full_lower, args_flat)\n\u001b[32m    148\u001b[39m   core.check_eval_args(args_flat)\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m   out_flat, compiled, profiler, const_args = \u001b[43m_pjit_call_impl_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m      \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    152\u001b[39m   out_flat = jit_p.bind(*args_flat, **p.params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py:1782\u001b[39m, in \u001b[36m_pjit_call_impl_python\u001b[39m\u001b[34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, donated_invars, ctx_mesh, name, keep_unused, inline, compiler_options_kvs, *args)\u001b[39m\n\u001b[32m   1770\u001b[39m \u001b[38;5;66;03m# Passing mutable PGLE profile here since it should be extracted by JAXPR to\u001b[39;00m\n\u001b[32m   1771\u001b[39m \u001b[38;5;66;03m# initialize the fdo_profile compile option.\u001b[39;00m\n\u001b[32m   1772\u001b[39m computation = _resolve_and_lower(\n\u001b[32m   1773\u001b[39m     args, jaxpr=jaxpr, in_shardings=in_shardings,\n\u001b[32m   1774\u001b[39m     out_shardings=out_shardings, in_layouts=in_layouts,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1780\u001b[39m     compiler_options_kvs=compiler_options_kvs,\n\u001b[32m   1781\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1782\u001b[39m compiled = \u001b[43mcomputation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[38;5;66;03m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n\u001b[32m   1785\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compiled._auto_spmd_lowering \u001b[38;5;129;01mand\u001b[39;00m config.enable_checks.value:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/pxla.py:2513\u001b[39m, in \u001b[36mMeshComputation.compile\u001b[39m\u001b[34m(self, compiler_options, device_assignment)\u001b[39m\n\u001b[32m   2510\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compilation_device_list, (\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), xc.DeviceList))\n\u001b[32m   2512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m compiler_options_kvs \u001b[38;5;129;01mor\u001b[39;00m device_assignment:\n\u001b[32m-> \u001b[39m\u001b[32m2513\u001b[39m   executable = \u001b[43mUnloadedMeshExecutable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_hlo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2514\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_hlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompile_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2515\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2516\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdevice_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompilation_device_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2517\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compiler_options_kvs:\n\u001b[32m   2518\u001b[39m     \u001b[38;5;28mself\u001b[39m._executable = executable\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/pxla.py:3059\u001b[39m, in \u001b[36mUnloadedMeshExecutable.from_hlo\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3056\u001b[39m       \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   3058\u001b[39m util.test_event(\u001b[33m\"\u001b[39m\u001b[33mpxla_cached_compilation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3059\u001b[39m xla_executable = \u001b[43m_cached_compilation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3060\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspmd_lowering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3061\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtuple_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_spmd_lowering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_prop_to_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3062\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_prop_to_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpmap_nreps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3065\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auto_spmd_lowering:\n\u001b[32m   3066\u001b[39m   \u001b[38;5;28;01massert\u001b[39;00m mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/pxla.py:2840\u001b[39m, in \u001b[36m_cached_compilation\u001b[39m\u001b[34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_kvs, pgle_profiler)\u001b[39m\n\u001b[32m   2832\u001b[39m compile_options = create_compile_options(\n\u001b[32m   2833\u001b[39m     computation, mesh, spmd_lowering, tuple_args, auto_spmd_lowering,\n\u001b[32m   2834\u001b[39m     allow_prop_to_inputs, allow_prop_to_outputs, backend,\n\u001b[32m   2835\u001b[39m     dev, pmap_nreps, compiler_options)\n\u001b[32m   2837\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dispatch.log_elapsed_time(\n\u001b[32m   2838\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFinished XLA compilation of \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{elapsed_time:.9f}\u001b[39;00m\u001b[33m sec\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2839\u001b[39m     fun_name=name, event=dispatch.BACKEND_COMPILE_EVENT):\n\u001b[32m-> \u001b[39m\u001b[32m2840\u001b[39m   xla_executable = \u001b[43mcompiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_or_get_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2841\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2842\u001b[39m \u001b[43m      \u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xla_executable\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/jax/_src/compiler.py:494\u001b[39m, in \u001b[36mcompile_or_get_cached\u001b[39m\u001b[34m(backend, computation, devices, compile_options, host_callbacks, executable_devices, pgle_profiler)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    493\u001b[39m   log_persistent_cache_miss(module_name, cache_key)\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_and_write_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m      \u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m      \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/jax/_src/compiler.py:762\u001b[39m, in \u001b[36m_compile_and_write_cache\u001b[39m\u001b[34m(backend, computation, executable_devices, compile_options, host_callbacks, module_name, cache_key)\u001b[39m\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compile_and_write_cache\u001b[39m(\n\u001b[32m    753\u001b[39m     backend: xc.Client,\n\u001b[32m    754\u001b[39m     computation: ir.Module,\n\u001b[32m   (...)\u001b[39m\u001b[32m    759\u001b[39m     cache_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    760\u001b[39m ) -> xc.LoadedExecutable:\n\u001b[32m    761\u001b[39m   start_time = time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m762\u001b[39m   executable = \u001b[43mbackend_compile_and_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    765\u001b[39m   compile_time = time.monotonic() - start_time\n\u001b[32m    766\u001b[39m   _cache_write(\n\u001b[32m    767\u001b[39m       cache_key, compile_time, module_name, backend, executable, host_callbacks\n\u001b[32m    768\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/jax/_src/profiler.py:364\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    363\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/jax/_src/compiler.py:378\u001b[39m, in \u001b[36mbackend_compile_and_load\u001b[39m\u001b[34m(backend, module, executable_devices, options, host_callbacks)\u001b[39m\n\u001b[32m    369\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m backend.compile_and_load(\n\u001b[32m    370\u001b[39m           built_c,\n\u001b[32m    371\u001b[39m           executable_devices=executable_devices,\n\u001b[32m    372\u001b[39m           compile_options=options,\n\u001b[32m    373\u001b[39m           host_callbacks=host_callbacks,\n\u001b[32m    374\u001b[39m       )\n\u001b[32m    375\u001b[39m     \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[32m    376\u001b[39m     \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[32m    377\u001b[39m     \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_and_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m xc.XlaRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    384\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m error_handler \u001b[38;5;129;01min\u001b[39;00m _XLA_RUNTIME_ERROR_HANDLERS:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from models.jax_vae_wavelet import VAE\n",
    "from polymer.load_tfrecords import Dataloader\n",
    "import jaxwt as jwt\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    pass \n",
    "    # batch_stats: Any\n",
    "    \n",
    "\n",
    "@jax.jit\n",
    "def train_step(\n",
    "    state_, \n",
    "    input_imgs_,\n",
    "    input_waves_, \n",
    "    rng_key,\n",
    "):\n",
    "    def loss_fn(params_):\n",
    "        (x_recon, x_wave, mu, log_var) = state_.apply_fn(\n",
    "            {'params': params_}, #'batch_stats': state_.batch_stats},\n",
    "            input_waves_,\n",
    "            # mutable=['batch_stats'],\n",
    "            training=True,\n",
    "            key=rng_key\n",
    "        )\n",
    "        \n",
    "        recon_loss = jnp.mean(jnp.abs(input_imgs_ - x_recon[..., jnp.newaxis]))\n",
    "        wave_loss = jnp.mean(jnp.abs(input_waves_ - x_wave))\n",
    "        \n",
    "        tot_err = recon_loss + wave_loss\n",
    "        \n",
    "        return tot_err, (tot_err)\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    aux, grads = grad_fn(state_.params)\n",
    "    tot_loss, (tot_loss_) = aux\n",
    "    \n",
    "    new_state = state_.apply_gradients(grads=grads)\n",
    "    # new_state = new_state.replace(batch_stats=updates['batch_stats'])\n",
    "\n",
    "    return new_state, tot_loss_\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def reconstruct_batch(state, input_waves, rng_key):\n",
    "    \"\"\"Get reconstructions for visualization\"\"\"\n",
    "    (x_recon, x_waves, mu, log_var) = state.apply_fn(\n",
    "        {'params': state.params},#, 'batch_stats': state.batch_stats},\n",
    "        input_waves,\n",
    "        training=False,\n",
    "        key=rng_key\n",
    "    )\n",
    "    return x_recon, x_waves, mu, log_var\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # GPU Check and Setup\n",
    "    print(\"🔧 Checking GPU availability...\")\n",
    "    print(f\"JAX devices: {jax.devices()}\")\n",
    "    print(f\"JAX default backend: {jax.default_backend()}\")\n",
    "    \n",
    "    try:\n",
    "        gpu_devices = jax.devices('gpu')\n",
    "        if gpu_devices:\n",
    "            print(f\"✅ Found {len(gpu_devices)} GPU(s): {gpu_devices}\")\n",
    "            print(\"🎯 Setting JAX to use GPU...\")\n",
    "        else:\n",
    "            print(\"⚠️ No GPU devices found - using CPU\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ GPU check failed: {e}\")\n",
    "        print(\"Using default JAX configuration\")\n",
    "    \n",
    "    compute_dtype = jnp.float32\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"Initializing VAE model...\")\n",
    "    vae = VAE(base_features=32, latent_dim=128)\n",
    "    key = random.PRNGKey(42)\n",
    "    x = random.uniform(key, (1, 128, 128, 4))\n",
    "    key, init_key, sample_key = random.split(key, 3)\n",
    "    variables = vae.init(init_key, x, sample_key, training=False)\n",
    "    print(\"Model initialized!\")\n",
    "    \n",
    "    # Training setup\n",
    "    key = random.key(0)\n",
    "    key, *subkeys = random.split(key, 4)\n",
    "    \n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 20\n",
    "    batch_size = 32\n",
    "    \n",
    "    print(f\"📋 Training Configuration:\")\n",
    "    print(f\"   Learning Rate: {learning_rate}\")\n",
    "    print(f\"   Epochs: {num_epochs}\")\n",
    "    print(f\"   Batch Size: {batch_size}\")\n",
    "    print()\n",
    " \n",
    "    params= variables['params']# , variables['batch_stats']\n",
    "    tx = optax.adamw(learning_rate)\n",
    "\n",
    "    state = TrainState.create(\n",
    "        apply_fn=vae.apply,\n",
    "        params=params,\n",
    "        tx=tx,\n",
    "        # batch_stats=batch_stats\n",
    "    )\n",
    "    \n",
    "    # Initialize dataloader\n",
    "    print(\"📂 Setting up dataloader...\")\n",
    "    dataloader = Dataloader(\n",
    "            tfrecord_pattern=os.path.join(dataset_path, \"*.tfrecord\"),\n",
    "            batch_size=batch_size,\n",
    "            enable_augmentation=True\n",
    "    )\n",
    "        \n",
    "    # Get training info\n",
    "    total_samples = dataloader.get_sample_count()\n",
    "    batches_per_epoch = dataloader.get_batches_per_epoch()\n",
    "    jax_ds = dataloader.get_jax_iterator()\n",
    "    print(\"Dataloader ready!\")\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    print(\"Starting training loop...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Adaptive Training Progress\"):\n",
    "\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        for batch_idx in range(batches_per_epoch):\n",
    "            batch_start = time.time()\n",
    "            \n",
    "            # Get batch\n",
    "            subkeys[2], key1, key2 = random.split(subkeys[2], 3)\n",
    "            batch_ = next(jax_ds)\n",
    "            perm = random.permutation(key1, batch_size)\n",
    "            \n",
    "            permuted_imgs = batch_['features'][perm, ...]\n",
    "            transformed = jwt.wavedec2(permuted_imgs, \"haar\", level=1, mode=\"reflect\", axes=(1,2))\n",
    "            # NOTE: this is because of the ordering in torch == BE CAREFUL\n",
    "            permuted_waves = jnp.concatenate([transformed[0], transformed[1][1], transformed[1][0], transformed[1][2]], axis=-1)\n",
    "            permuted_waves = jax.lax.stop_gradient(permuted_waves)\n",
    "            # Train step\n",
    "            state, total_loss = train_step(\n",
    "                state, permuted_imgs, permuted_waves, key2\n",
    "            )\n",
    "            \n",
    "            batch_time = time.time() - batch_start\n",
    "            \n",
    "            # print('LOSSES:', total_loss)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        # Show reconstructions every 5 epochs\n",
    "        if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "            key = visualize_reconstructions(state, permuted_imgs, permuted_waves, key, num_samples=4)\n",
    "        \n",
    "        # print(\"-\" * 60)\n",
    "    ckpt = {'model': state.params}\n",
    "    \n",
    "    import orbax\n",
    "    from flax.training import orbax_utils\n",
    "\n",
    "    orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "    save_args = orbax_utils.save_args_from_target(ckpt)\n",
    "    \n",
    "    # Remove the directory if it exists\n",
    "    checkpoint_path = '/code/checkpoints/vae'\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        shutil.rmtree(checkpoint_path)\n",
    "    \n",
    "    # Now save the checkpoint\n",
    "    orbax_checkpointer.save(checkpoint_path, ckpt, save_args=save_args)\n",
    "    print('Done -- saved to:', checkpoint_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2cfa29-aca0-463b-8996-c948a22a9611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
